% last updated in April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014

\documentclass[runningheads]{llncs}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
\usepackage{placeins}
\usepackage{graphicx}

\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{ruler}
\usepackage{color}
% \usepackage{subfigure}
% 
% \usepackage{setspace}
\usepackage{algorithm}
\usepackage{epsfig}
\graphicspath{{./Figure/}} 
% \usepackage[noend]{algorithmic}
% \newtheorem{thm}{Theorem}
\graphicspath{{./Figure/}} 
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\definecolor{darkblue}{rgb}{0,0,0.5}

\def\etal{et al.\ }
\def\ie{i.e.\ }
\def\eg{e.g.\ }

%put a wide hat over the argumetn
\newcommand{\lift}[1]{\ensuremath{\widehat{#1}}}

%put a wide hat over the argumetn
%\newcommand{\lifto}[1]{\ensuremath{\check{#1}}}
\newcommand{\lifto}[1]{\ensuremath{\overset{_{_{\circ}}}{#1}}}


% stack vector
\newcommand{\stackv}[1]{\ensuremath{\vet{v}\left( {#1} \right)}}
\newcommand{\ustackv}[1]{\ensuremath{\inv{\vet{v}}\left( {#1} \right)}}

% symmetric stack vector
\newcommand{\stackvs}[1]{\ensuremath{\vet{v}_{\textit{sym}}\left( {#1} \right)}}

% Matrix Lifting: put a wide hat over the argument intended to be a matrix
\newcommand{\mlift}[1]{\ensuremath{\lift{\mat{#1}}}}
\newcommand{\mlifto}[1]{\ensuremath{\lifto{\mat{#1}}}}

% Vector Lifting: put a wide hat over the argument intended to be a matrix
\newcommand{\vlift}[1]{\ensuremath{\lift{\vet{#1}}}}
\newcommand{\vlifto}[1]{\ensuremath{\lifto{\vet{#1}}}}

\newcommand{\bmat}[1]{\ensuremath{\begin{bmatrix} #1 \end{bmatrix}}}
% Vector: print the argument as a vector
\newcommand{\vet}[1]{\ensuremath{\mathbf{#1}}}

% Matrix: print the argument as a matrix
\newcommand{\mat}[1]{\ensuremath{\,\mathtt{#1}\,}}

% Inverse: print a -1 on the top right of the argument 
\newcommand{\inv}[1]{\ensuremath{{#1}^{\text{-}1}}}

% Inverse: print a -1 on the top right of the argument 
\newcommand{\minv}[1]{\ensuremath{\mat{{#1}}^{\text{-}1}}}

% Transpose: print a T on the top right of the argument 
\newcommand{\tra}[1]{\ensuremath{{#1}^{\!\mathsf{T}}}}

% Transpose Matrix: print a T on the top right of the argument intended to be a matrix 
\newcommand{\mtra}[1]{\ensuremath{\tra{\mat{#1}}}}

% Transpose Vector: print a T on the top right of the argument intended to be a vector
\newcommand{\vtra}[1]{\ensuremath{\tra{\vet{#1}}}}

% minus transpose:  print a -T on the top right of the argument
\newcommand{\ment}[1]{\ensuremath{{#1}^{\text{-}\mathsf{T}}}}

% minus transpose matrix:  print a -T on the top right of the argument
\newcommand{\mment}[1]{\ensuremath{{\mat{#1}}^{\text{-}\mathsf{T}}}}

% Cross Matrix:  print the argument in the cross matrix notation
\newcommand{\crmat}[1]{\ensuremath{\left[{#1}\right]_{\times}}}

\newcommand{\mattwoone}[2]{\ensuremath{\left( \begin{array}{c} #1 \\ #2 \\ \end{array} \right)}}

\newcommand{\mattwotwo}[4]{\ensuremath{\left( \! \begin{array}{cc} #1 & #2 \\ #3 & #4 \\ \end{array} \! \right)}}

\newcommand{\matthreethree}[9]{\ensuremath{\left( \begin{array}{ccc} #1 & #2 & #3 \\ #4 & #5 & #6 \\ #7 & #8 & #9 \\ \end{array} \right)}}

\newcommand{\Vi}{\ensuremath{\mat{V}_{\!i}}}
\newcommand{\VF}{\ensuremath{\mat{V}^{\!F}}}
\newcommand{\VB}{\ensuremath{\mat{V}^{\!B}}}
\newcommand{\VirtualF}[1]{\ensuremath{\hat{\mat{#1}}^{\!F}}}
\newcommand{\VirtualB}[1]{\ensuremath{\hat{\mat{#1}}^{\!B}}}
\newcommand{\VFi}{\ensuremath{\mat{V}^{\!F}_{\!i}}}
\newcommand{\VBi}{\ensuremath{\mat{V}^{\!B}_{\!i}}}
\newcommand{\VirtualFi}[1]{\ensuremath{\hat{\mat{#1}}^{\!F}_{\!i}}}
\newcommand{\VirtualBi}[1]{\ensuremath{\hat{\mat{#1}}^{\!B}_{\!i}}}
\newcommand{\Si}{\ensuremath{\mat{S}_{\!i}}}
\newcommand{\Center}{\ensuremath{\vet{C}}}
\newcommand{\mirror}{\ensuremath{\mat{\Pi}}}

\newcommand{\testmath}[1]{\mathrm{#1} \mathit{#1} \mathbf{#1} \mathsf{#1} \mathtt{#1} \mathcal{#1} \mathbb{#1} \mathfrak{#1}} 

\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


\newcommand{\todo}[1]{\fbox{\textcolor{red}{\textbf{ TODO: #1}}} \\}
\newcommand{\comment}[1]{{\small{\textsl{\textcolor{darkblue}{/* #1 */}}}}}
\newcommand{\draft}[1]{{\footnotesize{\textbf{\textcolor{blue}{Short version : compiling skipped}}}} \\}
\newcommand{\toadd}[1]{{\textbf{\textcolor{orange}{[ADD: #1]}}}}

%random variable
\newcommand{\rand}[1]{\ensuremath{\mathbb{#1}}}

%insert 2 figures on a row
\newcommand{\insertTwoF}[5]{
  \begin{figure}[h!]
    \centering
    \begin{minipage}{#4\linewidth}
    \includegraphics[width=\linewidth]{#1}
    \end{minipage}
    \begin{minipage}{#4\linewidth}
    \includegraphics[width=\linewidth]{#2}
    \end{minipage}
      \caption{#3}
      \label{#5}
  \end{figure}  
}

\newcommand{\insertF}[4]{
  \begin{figure}[h!]
    \centering
    \begin{minipage}{#3\linewidth}
    \includegraphics[width=\linewidth]{#1}
    \end{minipage}  
      \caption{#2}
      \label{#4}
  \end{figure}  
}

\newcommand{\thetab}{\boldsymbol\theta}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\nb}{\mathbf{n}}
\newcommand{\dt}{\mathrm{d}\mathbf{t}}
\newcommand{\gau}[3]{\exp(-\frac{1}{2#3}(#1-#2 )^2}
\newcommand{\wdist}[3]{\frac{1}{2#3}(#1-#2 )^2}
\newcommand{\bfE}{\mathbf{E}}
\newcommand{\bfz}{\mathbf{z}}

\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers
\pagestyle{headings}
\mainmatter
\def\ECCV14SubNumber{979}  % Insert your submission number here
\title{Automatic Building Detection Using Graph Cut}

\titlerunning{Méthodes avancées de traitement des images}

\authorrunning{SI343}

\author{LI Jia, XU Minmin}
\institute{Telecom ParisTech}


\maketitle

\section{Introduction}
The detection of buildings from remote sensed images (aerial \& satellite) is a very challenging problem which has many application, including urban monitoring, change detection, estimation of human population, etc. However, the huge amount of data make it impossible to process by manual method or semi automatic method (user intervention), that is why it is important to develop a automatic detection algorithm. The major difficulties are to take into account the different shapes of buildings during the detection and separate them from a complex background without any human intervention. \\
The goal of this framework is to study an origin approach proposed by Ok et al. \cite{OK:2013}, to implement the algorithm and test its performance. This method use the fact that buildings always cast shadows, to define a foreground-background selection. Such a selection allows to apply the grab cut segmentation (Rother et al., 2004 \cite{Rother:2004}).\\
In the section 1, we develop the grab cut algorithm and describe how to apply it to building detection; then we detail the automatic detection process, in which a shadow mask and a vegetation mask are processed to reveal the location of buildings. In section 3, we evaluate the performance of the method by defining the Precision, Recall and F1-score; most importantly, we explain the roles of all the involved parameters and test their impact on the model's performance. We discuss our result and point out the limitation and future work in section 4.
\section{Approach}
In this section, we show the proposed approach for automatic building detection. The method is based on the GrabCut segmentation algorithm, which is a semi automatic segmentation method producing a binary foreground background separation. The algorithm itself requires the user to provide a window indicating the foreground region for initialization. The idea is to provide such a window using the prior knowledge of the locations of buildings. More precisely, we use the fact that buildings cast shadows on the ground, and shadows are much easier to detect on images. We define a region of interest (ROI) for each shadow component and analysis the probability of the shadow belonging to a building, especially by eliminating the possibility of the object being a vegetation. In the case of a building, we extract the foreground information by searching along the solar elevation direction. And finally, we apply the GrabCut algorithm on a local window containing the ROI.\\
In the following part, we develop the GrabCut method, stressing on the theory. The we focus on the whole detection algorithm, and give the essential elements.
\subsection{Semi-automatic Segmentation : GrabCut}
The GrabCut method is a iterative segmentation, introduced by Rother et al. \cite{Rother:2004}, alternating between learning and an one-shot graph cut segmentation. In this part, we  first formalize the segmentation problem as a energy minimization problem. Given an image whose pixels are denoted by $\bfz=(z_1,z_2,...,z_N)$, where $N$ is the size of the image and $z_n$ could be one dimensional (gray level image) or multi dimensional (color image), a binary segmentation consist in finding a vector $\alpha=(\alpha_1,\alpha_2,...,\alpha_N)$, $\alpha_n\in\{0,1\}$, assigning all the pixels to the foreground or background according to a certain criterion. Typically, we want the labels $\alpha$ minimize an energy below:
\begin{equation}
\bfE(\alpha,\theta,\bfz)=U(\alpha,\theta,\bfz)+V(\alpha,\bfz)
\end{equation}
where,
  \begin{itemize}
  \item $\theta$ represents the stochastic model for each label/class, it could be either defined by the user or learned on the image (the empirical histogram, GMM...).
  \item $U=\sum_n -\log(p_\theta (\alpha_n,z_n))$ the likelihood term, it represents the data attachment term. 
  \item $V=\gamma \sum_{n,m\in \mathbf{C}}[\alpha_m \neq \alpha_n] \exp(-\beta|z_n-z_m|^2)$ the regularity term, it promote encourage the similar of neighbour pixels. $\beta$ and $\gamma$ are parameters of the algorithm. 
  \end{itemize}
Such an energy correspond to a Markov random field model for images, and for $\beta=0$ we have the classic Isings regularity term. It has been proved that such model is well adapted to image segmentation problem, and the global minimum of such an energy can be computed efficiently via a Graphcut algorithm (Kolmogorov, 2004 \cite{Kolmogorov:2004}).\\
The GrabCut replace this one shot GraphCut segmentation by an iterative procedure that alternate between estimation and parameters learnings. The foreground and the background are described by two Gaussian mixture models (GMM), each with K components. Hence, a additional parameter $\mathbf{k}=(k_1,k_2,...,k_N)$ is introduced, where $k_n$ assign each pixel $z_n$ to one of the GMM's components. The energy now becomes:
\begin{equation}
 \bfE(\alpha,\mathbf{k},\theta,\bfz)=U(\alpha,\mathbf{k},\theta,\bfz)+V(\alpha,\bfz)
\end{equation}
where,
 \begin{itemize}
  \item $U(\alpha,\mathbf{k},\theta,\bfz)=\sum_n D_n(\alpha,\mathbf{k},\theta,\bfz)$, with 
  \[
  \begin{split}
  D_n(\alpha,\mathbf{k},\theta,\bfz)&=-\log(p_\theta (\alpha_n,k_n,z_n)) -\log(\pi_\theta(\alpha_n,k_n)) \\
  &=-\log(\pi_\theta(\alpha_n,k_n))-1/2\log \det \Sigma(\alpha_n,k_n)\\
  &-1/2 \tra{[z_n-\mu(\alpha_n,k_n)]}\inv{\Sigma(\alpha_n,k_n)}[z_n-\mu(\alpha_n,k_n)]
  \end{split}
  \]

 \end{itemize}
we can see that $U$ correspond to the likelihood term of the GMM, and $\theta$ can be now written as:
\[
\theta=\{\pi(\alpha,k),\mu(\alpha,k),\Sigma(\alpha,k),\alpha \in \{0,1\},k\in \{1,...,K\}\}
\]
where $\pi$ is the weighting of the Gaussian component, and $\mu$, $\Sigma$ its means and covariance matrix. Given a predefined window $\mathbf{T}_u$, the algorithm can be summarized as following:
  \begin{enumerate}
   \item \textit{Initialization}. $\alpha_n=1$ for $n\in \mathbf{T}_u$, $\alpha_n=0$ otherwise.
   \item \textit{Learning}. $\mathbf{k}=\arg\min\limits_{\mathbf{k}} U(\alpha,\mathbf{k},\theta,\bfz)$, $\theta=\arg\min\limits_{\theta} U(\alpha,\mathbf{k},\theta,\bfz)$
   \item \textit{Estimation}. $\min\limits_{\alpha}\min\limits_{\mathbf{k}}\bfE(\alpha,\mathbf{k},\theta,\bfz) $
   \item \textit{Iteration}. Repeat 2,3 until convergence.
  \end{enumerate}
\subsection{Automatic Foreground-Background Windows Selection}
\insertF{schema}{Overview of the algorithm}{1}{sch}
\subsubsection{Shadow Mask}
To extract the building from the map by using the shadows, we need to extract the shadows first. \\
An easy and naive way to detect shadows is to apply a simple threshold on the gray scale image because intuitively, the shadows are the darkest regions in a map. However, this method would require human interference because the threshold would depend on the map we have and it could have much variation from one map to another. Thus, it is impossible to find a universal threshold for all maps and therefore this method is non-automatic.\\
Thus, an idea to automatize the threshold is to extract the threshold value from the image instead of giving the value by human. As we discussed above, the shadow correspond to the most dark regions in the gray scale image, therefore, by choosing the first peak in the histogram, we can extract the darkest regions in the map. \\
A method to realize this idea is the K-means. We need to give the number of clusters $n$ we want to get, and the algorithm would give the partition of the $n$ cluster finally. Then we use the limit value $\theta$ between the first and second cluster as the threshold we want. In this step, the number of clusters $n$ could change the result of partition and therefore the threshold $\theta$. In our experiences, 5 or 6 clusters would work well to find the good threshold.\\
\insertF{shadow_kmeans}{Detected shadow using k-means}{0.5}{1}
\FloatBarrier
However, this result is not good enough as we know that if the first two clusters are partly overlapped, then a part of shadow would be considered in the second cluster and thus will not be detected.\\ 
Since we have only used the gray scale information in this method, we could improve it by adding the geometric information. We will use the shadow detected in the first step as seed to be propagated by region-growing method. For each neighbour of each seed, we compare the difference between their gray scale values, and if the difference is less than our tolerance $\tau$, they are considered as shadow and we add them to the seed set. We repeat the process until there is no change in the seed set and we say the shadow is the seed set. In our tests, we choose $\tau =0.2$.\\
\insertF{shadow_croissance}{Detected shadow using region-growth}{0.5}{1}
Finally, we need to clean the shadow with the solar information. As the photo is taken at a specific time we can find the solar elevation angle $\phi$, the height $T_{height}$ and the directional information $\lambda$. The idea is to apply a morphological opening to each shadow component using a specific directional flat structuring element $\sigma_{L,lambda}$ generated from the known solar information: the directional information $\lambda$ with an edge length $L$. The edge length could be calculated by using $\phi$, $T_{height}$ and image resolution $c$. \\
$L=\lceil \frac{T_{height}}{tan \phi.c}\rceil$.\\
To construct the structuring element, we can use the Bresenham line discretion algorithm [Bresenham, 1965].\\
\insertF{se}{structuring element with $l=5$ and $\lambda=\frac{17}{12}\pi$}{0.3}{2}
We then apply a morphological opening to each shadow component using the structuring to eliminating the misclassification part of shadow that we introduced in the region-growing algorithm.\\
\insertTwoF{shadow_croissance}{shadow}{Comparison between shadow before and after cleaned by morphological opening}{0.4}{3}
\subsubsection{Vegetation Mask}
\subsubsection{Fuzzy Landscape}
We can model the spatial relation between buildings and their
shadows with a morphological fuzzy approach. Given a
shadow object $B$ and a non-flat line-based structuring element
$\nu_{L,\lambda,\sigma,\kappa}$, the landscape $\beta_{\lambda}(B)$ around the shadow object along the
given direction $\lambda$ can be defined as a fuzzy set of membership values in image space: \\
 $\beta_{\lambda}(B)=(B^{per}\oplus \nu_{L,\lambda,\sigma,\kappa})\cap B^C$. \\
Here, $B^{per}$ represents the perimeter pixels of the shadow object $B$ computed in 8-neighbourhood connectivity, $B^C$ is the complement of the shadow object $B$, and the operators $\oplus$ and $\cap$ denote the morphological dilation and a fuzzy intersection, respectively. The landscape membership values are defined in the range of $[0,1]$, and the membership values of the landscapes generated decrease when moving away from the shadow object and are bounded in a region defined by the object's extents and the direction defined by angle $\lambda$. The line-based non-flat
structuring element $\nu_{L,\lambda,\sigma,\kappa}$ is generated by combining the two structuring elements with a pixel-wise multiplication operator (*):
$\nu_{L,\lambda,\sigma,\kappa}=\nu_{L,\lambda}*\nu_{\sigma,\kappa}$ \\
Here, $\nu_{\sigma,\kappa}$ is a Gaussian non-flat structuring element with kernel size $\kappa$, and the rate of decrease of the membership values within the element is controlled by a single parameter $\sigma$. However, the flat
structuring element $\nu_{L,\lambda}$ is for providing directional information, where $L$ denotes the line segment and $\lambda$ is the angle where the line is directed.\\
\insertF{se_gaussian}{The line-based non-flat
structuring element $\nu_{L,\lambda,\sigma,\kappa}$}{0.3}{4}
\insertTwoF{local}{fuzzy_map_local}{Fuzzy landscape}{0.35}{5}
Finally, we apply a double threshold $\theta_1, \theta_2$ on the fuzzy landscape so that we can obtain a region which is inside the building. This result can be used as markers of the foreground for the following step of grab-cut. This method can also be used to eliminate the shadows of the vegetations by comparing the result of double threshold with the vegetation mask.\\
\subsubsection{Region of Interest}
Finally, in order to apply automatically the grab-cut algorithm, we need to define the background in the image. We can use an idea similar to thefore idea that we construct the fuzzy landscape.\\
We know that the shadow is always at side of the building with the certain solar direction $\alpha$. Thus, we can find the building in the reverse direction of the shadow, which inspire us to dilate the shadow in the reverse solar direction within certain distance $l$. Normally, our building is included in the region of dilated shadow, called ROI(region of interest). Here, the choice of $l$ is critic. If $l$ is too small, then we cannot include completely the building in our region of interest and therefore influence the accuracy of the grab-cut. 
\insertF{fore-back}{Foreground and background mask for grab-cut}{0.8}{6}
\FloatBarrier
\subsubsection{First Level Partition}


\section{Experience}
\subsection{Synthetic Test}
\subsection{Performance and Parameters' Influence}

\section{Conclusion}
\newpage

\bibliographystyle{splncs}
\bibliography{graph}
\end{document}
